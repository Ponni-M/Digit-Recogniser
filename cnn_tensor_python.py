# -*- coding: utf-8 -*-
"""cnn_tensor_python.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EnKdnZGU_MncLzrPg-Fns8PVZpOETiB-
"""

import math
import numpy as np
import matplotlib.pyplot as plt
import scipy
import tensorflow as tf
from keras.datasets import mnist
           
def dense_to_one_hot(labels_dense, num_classes=10):
  """Convert class labels from scalars to one-hot vectors."""
  num_labels = labels_dense.shape[0]
  index_offset = np.arange(num_labels) * num_classes
  labels_one_hot = np.zeros((num_labels, num_classes))
  labels_one_hot.flat[index_offset + labels_dense.ravel()] = 1
  return labels_one_hot

tf.reset_default_graph()

#create placeholder
X = tf.placeholder(tf.float32, [None, 28, 28, 1])
Y = tf.placeholder(tf.float32, [None, 10])
  
#create variable
W1 = tf.get_variable("W1", [4, 4, 1, 8], initializer=tf.contrib.layers.xavier_initializer(seed=0))
W2 = tf.get_variable("W2", [2, 2, 8, 16], initializer=tf.contrib.layers.xavier_initializer(seed=0))
  
#forward propogation
Z1 = tf.nn.conv2d(X, W1, strides=[1, 1, 1, 1], padding='SAME')
A1 = tf.nn.relu(Z1)
P1 = tf.nn.max_pool(A1, ksize = [1, 8, 8, 1], strides = [1, 8, 8, 1], padding='SAME')
Z2 = tf.nn.conv2d(P1, W2, strides=[1, 1, 1, 1], padding='SAME')
A2 = tf.nn.relu(Z2)
P2 = tf.nn.max_pool(A2, ksize = [1, 4, 4, 1], strides = [1, 4, 4, 1], padding='SAME')
P = tf.contrib.layers.flatten(P2)
Z3 = tf.contrib.layers.fully_connected(P, 10, activation_fn=None)

# cost calculation
cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=Z3, labels=Y))

#optimizer
learning_rate = 0.01
optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)
  
#predictor
predict_op = tf.argmax(Z3, 1)
correct_prediction = tf.equal(predict_op, tf.argmax(Y, 1))
accuracy = tf.reduce_mean(tf.cast(correct_prediction, "float"))

#train
init = tf.global_variables_initializer()
costs = []
num_epochs = 100

(X_train, Y_train), (X_test, Y_test) = mnist.load_data()
X_train = np.expand_dims(X_train, axis=3)
Y_train = dense_to_one_hot(Y_train, 10)
X_test = np.expand_dims(X_test, axis=3)
Y_test = dense_to_one_hot(Y_test, 10)

(m, _, _, _) = X_train.shape
print (X_train.shape)
subsetSize = 2000
with tf.Session() as sess:
  sess.run(init)
  for epoch in range(num_epochs):
    _ , temp_cost = sess.run([optimizer, cost], feed_dict={X:X_train[1:subsetSize], Y:Y_train[1:subsetSize]})
    costs.append(temp_cost)

  #print cost
  plt.plot(np.squeeze(costs))
  plt.ylabel('cost')
  plt.xlabel('iterations (per tens)')
  plt.title("Learning rate =" + str(learning_rate))
  plt.show()
    
  index = 10
  print(predict_op.eval({X: X_train[index:index+1, :], Y: Y_train[index:index+1, :]}))
  #my_image = scipy.misc.imresize(np.squeeze(X_train[index]), size=(28,28))
  my_image = X_train[index].reshape(28,28)
  
  plt.imshow(my_image)
 
  train_accuracy = accuracy.eval({X: X_train[1:subsetSize], Y: Y_train[1:subsetSize]})
  test_accuracy = accuracy.eval({X: X_test[1:100], Y: Y_test[1:100]})
  print("Train Accuracy:", train_accuracy)
  print("Test Accuracy:", test_accuracy)

